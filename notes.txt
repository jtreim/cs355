SD: 720 x 576
HD: 1280 x 720
Full HD: 1920 x 1080
Quad HD: 3840 x 2160
4K/2K: 4096 x 2160

____Interlacing________
Progressive: full screen 60 fps
Interlaced: 
    Half screen fields 60 times/second. (Odd lines, then even lines...). Equivalent of 30 fps.

____Rasterization________
- Vector graphics are higher quality, but everyone uses raster displays.
- Converting to raster image first: called rasterization (sometimes scan conversion).

Postscript: When sending to printer, we send a program of how to draw a page, not the actual image.

Alpha channel included in the last byte per pixel. (0 = transparent, 255 opaque).

____Handling Buffers________
Single buffer: Erase/redraw only what's changed. Uses clipping. Determining what's in that area
               is usually not worth it. Might lead to flickering, but only in areas being updated.
Double buffer: Don't really draw to the real screen buffer. Draw to offscreen buffer.
               Copy buffers (fast). Some systems support switching with just pointers. Most common
               for games, animations, etc.

In most systems there are lots of layers:
- Drawing API
- GUI
- OS
- Graphics drivers
- Graphics cards
- displays



________________Level (Point) Operations________________
General form:
- r = input value
- s = output value
- T = a grey level transformation

for all pixel positions x, y:
out[x,y] = function(in[x,y])

brightness: s = r + c. c > 0: lighter. c < 0: darker. c: bias/offset.
contrast: s = a * r. a > 1: more contrast. a < 1: less contrast. a: gain.
clipping to a limited range:
    /-- s_min if r < s_min
s --- s_max if r > s_max
    \__ r otherwise

Scaling:
    s = (r - r_min)*(s_max - s_min)/(r_max - r_min) + s_min
Negative:
    s = r_max - r
        or
    s = r_max - r + r_min
Quantization:
    s = s_n if r_n <= r <= r_max
Logarithm/Exponent:
    Sometimes care more about relative changes than absolute ones.
    Lots of things use logarithmic scales (s = log(r)).
    - Decibel (dB) units
    - Apparent brightness
    - Richter scale
    - Human Vision
    Can "undo" with exponentiation
Power Functions: Can also raise to a desired power.
    s = r^p
    Used in gamma correction. (I = V^(some constant) + c)
    

____________Color________________________
- Cones have 3 different kinds of color-sensitive pigments, each responding to a different
  range of wavelengths.
- These are roughly "red", "green", and "blue".
- The sensitivity and number of the three types of cones are different.
- More sensitive overall to green and red than to blue.
- Technically, there is a difference between:
    -> Physical intensity
    -> Perceptual brightness
- Caused by differences in sensitivity of our eyes to different wavelengths.
- Luminous efficiency function

Color Models:
- Color is a natural phenomenon, but for graphics and imaging we need to represent colors numerically.
- Many ways to do this.
- Numerical representations of color are called color models.
- Simplest are RGB. Can be thought of as points in a cube.
- Primaries and secondaries are complementary.
Primary: ones mixed to make other colors.
Secondary: Pairwise combinations of primaries.
- Can be additive or subtractive

Additive: RGB
Subtractive: CMY(K). K is the "pure black" as the fourth primary.

Color Gamuts:
- Can visualize the space of all visible colors using a chromaticity diagram.
- No 3 primaries can span the space of visible colors.


________________Luminance and Chromaticity________________________________
- RGB model is common but not all that intuitive to use.
- Most common for Chromaticity:
    -> Luminance: how bright/strong a color is. Equivalent of measuring
                  quantity of light independent of wavelength. Most
                  contributes to human perception of shape and form. Same
                  as "value" in art.
    -> Hue: What we first think of as "color". Pure wavelength of light.
    -> Saturation: how pure a color is.

HSI color model:
    Mapping all colors to a double-ended cone.
    Angle = hue. Distance from center = saturation. Axis down the middle 
    is the "line of grays".
HSV color model:
    Single ended-cone. Like HSI but only one cone. Hue-Lightness-Saturation,
    Hue-Value-Chroma both same idea.
NTSC (National Television Standards Committee) YIQ Model:
    Chromaticity requires two parameters, but these don't have to be hue
    and saturation. Lots of other variations.
    Y = Luminance, I and Q = chromaticity.
Others:
    CIE LUV, CIE La*b* (attempts to be perceptually linear), YCrCb (used
    in JPEG standard).

Color Image Processing:
- Common approach:
    -> Convert to an appropriate model such as HSI, etc.
    -> Process
        Brightness/contrast adjustments
        Preserving or shifting the hues
        Adjusting the saturation
    -> convert back to RGB if needed.

Gamut: Space of colors spanned by the primaries of the device.


________________Image Arithmetic________________________________
Addition:
out(x,y) = (alpha1)in1(x,y) + (alpha2)in2(x,y) -- Adding fraction
                                                  of each to avoid
                                                  maxed values.
Subtraction:
- Useful for finding changes between images
- Often more useful to use absolute difference.
- Digital Subtraction Angiography:
    1. Take an x-ray
    2. Inject patient with radio-opaque dye ("don't move!")
    3. Take another x-ray
    4. Subtract the two.
- Motion: use differencing to identify motion in an otherwise
          unchanging scene (object motion, not camera motion)
          Basis for motion tracking techniques in computer vision.
          Use overall shift (minimum difference) for tracking
          camera motion. Part of a larger process called a
          "match move" in film making. Essential for inserting CGI
          into a real scene with a moving camera (the virtual camera
          has to move the same way the physical camera did).
          Useful for video compression. Only encode the difference
          between frames. Motion detection/prediction used in video
          compression (MPEG, etc).

Image averaging:
- Average multiple pictures of the same static scene to reduce noise.
- Similar in principle to acquiring the image for a longer duration.

Bitwise And/Or:

Alpha Blending:
- Use per-pixel weights to blend two images:
    out(x,y) = (alpha1)(x,y)in1(x,y) + (alpha2)(x,y)in2(x,y)
    Commonly:
    out(x,y) = (alpha)(x,y)in1(x,y) + (1- (alpha)(x,y))in2(x,y)
- Blending often uses an alpha mask (aka matte).

Neighborhood operations:
- Output pixel value is a function of that pixel and its neighbors.
- Possible operations: sum, weighted sum, average, weighted average, min, max, median, ...
           (I(x-1, y-1), I(x, y-1), I(x+1, y-1))
I'(x,y) = f(I(x-1,y), I(x,y), I(x+1, y))
           (something else...)

Pixel Grid - Neighbors?
- 4-connected (N,S,E,W)
- 8-connected(add NE, SE, SW, NW)

Pixel Grid - Distance:
- Euclidean distance: Pythagorean theorem.
- 4-connected steps: "city block", "Manhattan". Sum of differences between x & y.
- 8 - connected steps: "chessboard". Larger of differences between x & y.

Spatial Filtering:
- Most common is to multiply each of the pixels in the neighborhood by a
  respective weight and add them together.
- Local weights are called a mask or kernel.

- Giving information to neighbors: Correlation.
- Getting information from neighbors: Convolution. Basically the same, but flipped mask.

Notation for convolution operator: I' = I * w
                                          ^
                                        Not multiplication, convolution.
What to do when neighbor lies outside the image boundaries?
 - Be consistent with it, pick what makes sense for operation.
    - Assume zero or some other constant (average of image).
    - wrap around.
    - Don't do edges.
    - Assume same as its neighbor inside image.

Smoothing:
- If we can average multiple images together to remove noise, why not average multiple pixels?
- Assumption - Correct values are all the same.
- Effects: Reduces noise, causes blurring on edges. Main purpose is to reduce noise.
- Any kernel with all positive weights does smoothing/blurring.
- To average rather than add, divide by the sum of the weights.
- Can be any size (larger means more blurring)

Nonlinear Smoothing:
- Spatial filtering is linear, byt many neighborhood operators are not.
- Some do noise reduction;
    - Trimmed mean
    - Median filter
    - Bilateral filtering (or other adaptive weights)
- These try to be less sensitive to outliers and/or respect edges.

Median filtering:
- Output is the median (not the mean) of the neighborhood pixels
- More robust to outliers (great for "salt and pepper" noise)
- Tries to respect edges (goes with local majority)
- But often rounds corners or loses very small/thin things

Bilateral Filtering:
- Spatially adapt the weights of the mask.
- Idea is to average with neighbors, but respect edges. If it's already similar, average.
  Otherwise, don't or don't near as much.
- Close neighbors get more weight.
- Similar neighbors get more weight.
- Computationally expensive. Pretty effective at removing noise though.

Negative Weights:
- Requires a mix of positive and negative weights to do sharpening.

Unsharp Masking:
- Key idea: mask (subtract) out the blur
Procedure:
    - Blur more
    - Subtract from original
    - Multiply by some fraction
    - Add back to the original
Mathematically: I' = I + (alpha)(I - ~I)
    - I = input image
    - ~I = Blurred input image
    - (alpha) = Weighting (controls sharpening)
    - I' = Output image

Normalizing a vector:
v-hat = v/||v|| = v/sqrt(v_1^2 + v_2^2 + ... + v_n^2)
0 vector has a length of 0, and an undirected angles.

Geometric Interpretation: the dot product of a vector and a unit vector is the length
                          of the projection onto that unit vector.

Orthogonality: vectors whose dot product is zero are said to be "orthogonal"
               "Right angle" to each other (regardless of length)

Dot product between vectors: scalar - single number. Works across all dimensions.
Cross product between vectors: only done in 3D.
                               AxB is orthogonal to A & B.
                               Sometimes get a right-handed coordinate system answer.
                               Sometimes get a left-handed coordinate system answer.

Translation: p` = p + t (point plus translation vector)
Rotation: p`_x = p dot ^e_x

Vectors are simply an n x 1 matrix
Transposing: swap rows for columns.
A matrix is an n x m array of numbers, or a matrix is a stack of transposed vectors, each with m elements.
Matrix multiply is simply a lot of dot products in parallel

Column vectors: CBAv = C(B(Av))       Row vectors: vABC = (((vA)B)C)

    Line 1:                          Line 2:
a_1x + b_1y = d_1               a_2x + b_2y = d_2


- Rank of a matrix is the number of linearly independent rows.
- When used as transforms, matrices with full rank transform to full space.
- Singular matricies have insufficient rank and collapse to a corresponding subspace.
- Geometric interpretation: the rank of a matrix is the dimensionality of the (sub)space that matrix maps to.

- A square matrix are orthogonal iff MM^T = I
- Implies rows are orthonormal vectors. Implies that M^T == M^-1
- All rotation matrices are orthogonal and all orthogonal matrices are rotations!

________________________3D Rendering____________________________________________
Projection: To get 2D pictures of a 3D world, you have to use projection.
            - Orthographic: Simply dropping a dimension. Used in technical drawings, etc.
            - Perspective: Many graphics systems assume a simple pinhole camera model.

focal (f)
length  /
|  |   /
V  V  /
|\   /|<--virtual imaging plane
| \ / |
|--X--|----optical axis
| /^\ |
|/ | \|
 focal\
point  \
        \
        field of view


Camera Coordinates: Projection
           P(X, Y, Z)
      |   / 
      |  /|    
focal | / |<--similar triangles: x/f = X/Z -> only determines where it physically falls on the sensor,
point |------- optical axis                   not where the point should be in terms of pixels.
      |   |
      |   |
      |  virtual imaging plane: imaginary plane to simplify flipping the image.
(x, y) = (fX/Z, fY/Z)

_a = alpha 
 [[x],  [[_ax],
  [y], ~ [_ay],
  [1]]   [_a]]
 
 [[x],   [[fX/Z],   [[X],    [[1,0,0,0],    [[X],
  [y], =  [fY/Z], ~  [Y],  =  [0,1,0,0],  *  [Y],
--[f],----[-f-],-----[Z],-----[0,0,1,0],-----[Z],-- Some formats drop this row.
  [1]]    [ 1 ]]    [Z/f]]   [0,0,1/f,0]]    [1]]

---Polygons!!---
Vertex: 3D point (X,Y,Z)
Edge: Line connecting 2 vertices.
Face: Polygon defined by a set of "adjacent (connected by edges) vertices.

Storage:
- List of vertices
- List of faces bound by vertices (by index)
- Other information about vertices/faces.
Avoids duplication.

Example:
# List of vertices
v 0.123 0.234 0.345 1.0
v ...

# List of faces
f 1 2 3
f 3 4 5
f 6 3 7

Normals:
- It's useful to determine the normal to the polygonal face.
 -> Visibility
 -> Lighting
 -> ...
- Be consistent--usually go with outward facing.


World space defines the space in which objects can live. Origin and coordinate system is arbitrary.
Object space: Coordinate system used to define an object. Usually chosen to make object definition
              the simplest.
Placing an object in the world defines an object-to-world transformation.
Order of operations: 1. Scale
                     2. Rotate
                     3. Translate
World to Camera: 1. Translate everything relative to the camera position.
                 2. Rotate into the camera's viewing orientation.

[[x],   [[X_c],    [[1,0,0,0],    [[e_11,e_12,e_13,0],   [[1,0,0,-c_x],   [[X_w],
 [y], ~  [Y_c],  =  [0,1,0,0],  *  [e_21,e_22,e_23,0], *  [0,1,0,-c_y], *  [Y_w],
 [f],    [Z_c],     [0,0,1,0],     [e_31,e_32,e_33,0],    [0,0,1,-c_z],    [Z_w],
 [1]]   [Z_c/f]]   [0,0,1/f,0]]        [0,0,0,1]]         [0,0,0,1]]        [1]]

Field of View: Spend as little time as possible on things that are outside the field of view.

Near & Far planes: don't render things that are too small to be viewable.


OpenGL key matrices:
- ModelView (converts from model coordinates to camera ones)
    -> Object to world
    -> World to view
- Projection
    -> Orthographic
    -> Perspective
- Viewport                      -------------- Model View ----------------- 
                                |                                         |
3D Geometry Pipeline:           v                                         v
Object Coordinates ---> Object to World ---> World Coordinates ---> World to Camera --->
Camera Coordinates ---> Perspective Projection (Projection) ---> Imaging Plane Coordinates --->
Viewport Mapping (Viewport) ---> Screen Coordinates

glMatrixMode: Changes which matrix you're manipulating
glLoadIdentity: Loads the identity matrix as the current one.
glRotated: Concatenates a rotation matrix to the current one.
glTranslated: Concatenates a translation matrix to the current one.
glOrtho: loads an orthographic projection matrix.
gluPerspective: loads a perspective projection matrix.

M = RT:
M <- I
M <- T
M <- R

Object Hierarchies:
            Sun
           /   \
        Earth  Mars
        /      /  \
     Moon  Phobos Deimos
Mars <- Earth: Earth relative to sun matrix * mars relative to sun inverse matrix

                            World
                              |
                            Campus
                     ______/  |  \_______
                Building    Building    Building
                      _____/  |   \____
                    Room   __Room_     Room
                          /   |   \
                     Chair  Chair  Chair


3 common ways to do Occlusion Testing:
    1. Ordered rendering (painter's algorithm):
        Render from back to front. Draw things over top of others. Problem: Polygon depth isn't
        strictly ordered. Interpenetration. Mutually overlapping.
    2. Image space testing (Z-buffering): Only override pixels if closer to camera than what's
        already there. Problems: How to handle ties? Quantization of the finite-precision z buffer.
        Round-off error may be an issue. Nonlinear by depth.
    3. Ray Casting: Shoot a ray out from the camera's focal point through the pixel location.
        What does it hit first? Lots of ray-primitive intersection tests.



____________________________Lighting & Shading________________________________________________
Kinds of Lighting:
 - Direct: Light falling on an object directly from a light source.
 - Indirect: Light falling on an object after being reflected off (or going through) other objects.
 - Ambient: General light bouncing around and scattered enough to be effectively "everywhere".

Light Sources:
 - Point
 - Area
 - Spot
 - etc.

Basic Geometry of Lighting:
 - Perpendicular to surface: Surface normal (n)
 - Lighting direction (to the light): l
 - Viewing direction: Viewing direction to the eye/camera (v)
 - Reflected light direction (r)

Surface Reflectance:
 - Most objects don't give off light.
   -> Reflect some of the light that falls on them
   -> Absorb the rest
 - The wavelengths relfected give the object its color.
 - The effect if multiplicative: i.e. "reflects 40% of the green light"
 - If modeled as RGB, we can also model reflectance as RGB
 - Reflectance is also sometimes called albedo.

Irradiance:
 - We sometimes say "the amount of light"
 - It's really how much light per unit area
 - This quantity is called the irradiance
 - Two important properties:
   -> Irradiance falls off with the square of the distance
   -> Irradiance is less when falling on a slanted surface.
 
Specular vs Diffuse:
 - Some light is reflected perfectly (specular)
 - Some light is scattered (diffuse)

Diffuse Reflection:
 - Light scattered in every direction is called the diffuse part of the reflected light.
 - A perfectly diffuse surface is called Lambertian (chalk, moon are examples)
 - Only lighting direction matters
 - Viewing direction does not

c_diff = (s (X) m_diff)(n dot l)
(X) = pointwise multiply RGB
c_diff = diffuse reflected color
s = source intensity
m_diff = material diffuse reflectance
if n dot l = 1: means surface is pointing right at light
if n dot l < 0: means surface is pointing away from light. Floor it to 0.
if n > 0 & n < 1: means surface is towards light, but not directly at it.

Specular Reflections:
Angle of reflection = Angle of incidence (but may be blurred)
c_spec = (s (X) m_spec)(v dot r)^m_gls
c_spec = specular reflected color
s = source intensity
m_spec = material specular reflectance
m_gls = how glossy the surface is (the higher it is, the less the reflection deviates, meaning
         it is less diffused light)
v dot r is also clipped to 0

Computing the reflection direction:
r + l = lies in surface normal.
r + l = 2(l dot n)n, so r = 2(l dot n)n - l


Phong model: specular lighting + diffuse lighting + ambient lighting. Really good at plastic/polished
             model.
Blinn model is similar with slightly different specular
        ^^^^^^^^^^^^^^^^^^^^^^^^^
Modelling this light by one light source. To model with multiple light sources, just repeat and add.

BRDFs: Phong model is only an approximation
       Real reflections are not a simple mix of pure diffuse and pure specular
       Function of both incoming direction and outgoing direction
       Reflectance isn't constant across the surface
BRDF (Bi-direction reflectance distribution function): f(x, w_1, w_2, gamma)
                                                position-^   ^    ^     ^-wavelength
                                                         in__|    |__out

Shadows:
 - Point lights cast hard shadows
 - Area lights cast softer shadows
 - umbra = area in full shadow
 - penumbra = area in partial shadow

Simple shadows:
 - For point lights, shadows are pretty simple.
 - Do a visibility test from the point of view of the light
 - Z-buffering can also be used for distance-based falloff

Flat shading:
 - The entire polygon has the same normal, so it's all colored the same.
 - Leads to "flat shading"

Gouraud Shading:
 - Simple idea:
   -> Compute shading at edges/vertices and interpolate.
   -> Works well for diffuse
   -> Doesn't work as well for specular
Phong Shading:
 - Key idea:
   -> Instead of interpolating the shading, interpolate the normals
   -> Compute shading on a per-pixel basis using interpolated normal

Bump Maps (more commonly called normal mapping):
 - Texture maps are only reflectance
 - They don't respond to changes in the lighting ("painted on")
 - Idea: use a normal map. Essentially define normals of points on the model.

Ray Tracing (Turner Whitted, 1980):
 - Handles reflections, refractions
 - Shoot a ray out through the pixel on the imaging plane. When it hits something,
   recursively shoot more rays:
     -> Towards light source(s)--direct lighting
     -> Reflection ray (if applicable)
     -> Refraction ray (if applicable)

Diffuse interreflections
Radiosity:
 - Math involves integrals over all angles of light coming into and off of a surface.
 - Approximated using importance sampling
 - Effectively traces lots and lots of rays--compute intensive
 - Because radiosity involves diffuse light, it is independent of the view.

Forward rendering:
 - Some phenomena can't be modeled well by going "backward" from the camera.
 - Some require "forward" rendering of the light from the light source. E.g. "caustics"
   (Light refracts through some medium, reflects and illuminates som other surface).
 - forward/hybrid methods:
   -> Path tracing
   -> Metropolis light transport
   -> Photon mapping
   -> All try to approximate the "rendering equation" (Jim Kajiya, 1986)

Signals as functions:
 - Digital signals can be thought of as sampled functions
 - Domains:
   -> Time (audio)
   -> Space (images)
   -> Both (video)
 - Ranges:
   -> Changing air pressure (audio)
   -> Visible light (photographs, video)
   -> Other properties (X-rays, MRI, range images, etc.)

Sampling vs Quantization:
 - Sampling: Discretization of the domain (conversion of domain to digital format)
   -> Samples per unit length, area, etc.
   -> Often expressed as:
     -- rate
     -- spacing
     -- density
     -- 600 dots per inch, 44.1 KHz, or 30 frames per second
 - Quantization: Discretization of range (conversion of range to digital format)
   -> Levels of precision in each sample
   -> Usually
     -- number of levels
     -- number of bits
     -- black and white images, 8-bit grey, 16-bit audio

Acquisition Devices:
 - Aperture <--\
 - Scanning     |--focusing on these two
 - Sensor <----/
 - Quantizer
 - Output storage medium

Apertures (window through which a device is seeing the world):
 - Pixels aren't point samples.
 - Total light over an area of the visible scene.
 - Controlled by the camera's iris (photographers: F-stop)
 - Also caused by physical sensor area (pixel's area on the camera's sensor)
 - Sampling and size of aperture determine resolution.
   -> Smaller apertures = better resolution
   -> Larger apertures = worse resolution
 - Lenses allow a physically larger aperture to act as an effectively smaller one

Resolution: Ability to discern detail
Measuring Resolution:
 - One way is to use alternating black/white lines with fixed spacing
   -> Increase density until you can't see the separate lines.
     -- Gradually blurs to grey
     -- Stop when half the original contrast
   -> Units: line pairs per millimeter

